{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25635c5-6770-4c2d-bc0b-9ce1070ccf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== Classification Model with Ensemble ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# --- Load dataset ---\n",
    "df = pd.read_csv(\"C:\\Users\\C Sutharsan\\Downloads\\GUVI class notes AIML\\Capstone_project\\Project_4\\Final\\final_dataset.csv\")\n",
    "\n",
    "# Advanced feature engineering for visit mode prediction\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create sophisticated features that better predict visit mode\"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. USER BEHAVIOR PATTERNS\n",
    "    # User's preferred travel seasons\n",
    "    user_seasonal_preference = df.groupby(['UserId', 'VisitSeason']).size().reset_index(name='count')\n",
    "    user_dominant_season = user_seasonal_preference.sort_values('count', ascending=False).groupby('UserId').first()\n",
    "    df_enhanced = df_enhanced.merge(user_dominant_season[['VisitSeason']], left_on='UserId', right_index=True, how='left', suffixes=('', '_preferred'))\n",
    "    \n",
    "    # User's visit mode percentages - fixed for pandas warning\n",
    "    user_visit_mode_freq = df.groupby(['UserId', 'VisitMode_y']).size().reset_index(name='count')\n",
    "    user_visit_totals = user_visit_mode_freq.groupby('UserId')['count'].sum().reset_index(name='total')\n",
    "    user_visit_mode_freq = user_visit_mode_freq.merge(user_visit_totals, on='UserId')\n",
    "    user_visit_mode_freq['pct'] = user_visit_mode_freq['count'] / user_visit_mode_freq['total']\n",
    "    user_visit_mode_pct = user_visit_mode_freq.pivot(index='UserId', columns='VisitMode_y', values='pct').fillna(0)\n",
    "    \n",
    "    for col in user_visit_mode_pct.columns:\n",
    "        df_enhanced = df_enhanced.merge(user_visit_mode_pct[[col]], left_on='UserId', right_index=True, how='left')\n",
    "        df_enhanced.rename(columns={col: f'user_pct_{col}'}, inplace=True)\n",
    "    \n",
    "    # 2. TEMPORAL PATTERNS\n",
    "    # Visit mode by month patterns\n",
    "    monthly_mode_dist = pd.crosstab(df['VisitMonth'], df['VisitMode_y'], normalize='index')\n",
    "    for mode in monthly_mode_dist.columns:\n",
    "        month_mode_prob = dict(zip(monthly_mode_dist.index, monthly_mode_dist[mode]))\n",
    "        df_enhanced[f'month_mode_prob_{mode}'] = df_enhanced['VisitMonth'].map(month_mode_prob)\n",
    "    \n",
    "    # Cyclical encoding for months\n",
    "    df_enhanced['sin_month'] = np.sin(2 * np.pi * df_enhanced['VisitMonth'] / 12)\n",
    "    df_enhanced['cos_month'] = np.cos(2 * np.pi * df_enhanced['VisitMonth'] / 12)\n",
    "    \n",
    "    # 3. GEOGRAPHIC PATTERNS\n",
    "    # Visit mode by demographic patterns\n",
    "    continent_mode_dist = pd.crosstab(df['Continent'], df['VisitMode_y'], normalize='index')\n",
    "    \n",
    "    for mode in continent_mode_dist.columns:\n",
    "        continent_mode_prob = dict(zip(continent_mode_dist.index, continent_mode_dist[mode]))\n",
    "        df_enhanced[f'continent_mode_prob_{mode}'] = df_enhanced['Continent'].map(continent_mode_prob)\n",
    "    \n",
    "    # 4. INTERACTION FEATURES\n",
    "    # User-Attraction compatibility scores\n",
    "    user_attraction_compatibility = df.groupby(['UserId', 'AttractionType'])['Rating'].mean().reset_index()\n",
    "    df_enhanced = df_enhanced.merge(\n",
    "        user_attraction_compatibility.rename(columns={'Rating': 'user_attraction_compatibility'}),\n",
    "        on=['UserId', 'AttractionType'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 5. BEHAVIORAL SEQUENCES\n",
    "    # Previous visit mode\n",
    "    df_enhanced = df_enhanced.sort_values(['UserId', 'VisitYear', 'VisitMonth'])\n",
    "    df_enhanced['prev_visit_mode'] = df_enhanced.groupby('UserId')['VisitMode_y'].shift(1)\n",
    "    \n",
    "    # 6. ADVANCED AGGREGATIONS\n",
    "    # User travel diversity score\n",
    "    df_enhanced['user_travel_diversity'] = df_enhanced.groupby('UserId')['VisitMode_y'].transform('nunique')\n",
    "    df_enhanced['user_attraction_diversity'] = df_enhanced.groupby('UserId')['AttractionType'].transform('nunique')\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_columns = df_enhanced.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        df_enhanced[col] = df_enhanced[col].fillna(df_enhanced[col].mean())\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Apply enhanced feature engineering\n",
    "df_enhanced = create_advanced_features(df)\n",
    "\n",
    "# Calculate month-mode probabilities\n",
    "monthly_mode_dist = pd.crosstab(df_enhanced['VisitMonth'], df_enhanced['VisitMode_y'], normalize='index')\n",
    "\n",
    "# Convert to nested dict: {mode: {month: prob, ...}, ...}\n",
    "month_mode_probs = {mode: monthly_mode_dist[mode].to_dict() for mode in monthly_mode_dist.columns}\n",
    "\n",
    "# Calculate continent-mode probabilities\n",
    "continent_mode_dist = pd.crosstab(df_enhanced['Continent'], df_enhanced['VisitMode_y'], normalize='index')\n",
    "\n",
    "# Convert to nested dict: {mode: {continent: prob, ...}, ...}\n",
    "continent_mode_probs = {mode: continent_mode_dist[mode].to_dict() for mode in continent_mode_dist.columns}\n",
    "\n",
    "# Define features\n",
    "numerical_features = [\n",
    "    'VisitMonth', 'VisitQuarter', 'VisitYear',\n",
    "    'continent_mode_prob_Business', 'continent_mode_prob_Couples',\n",
    "    'user_pct_Couples', 'user_pct_Family', 'user_pct_Friends', 'user_pct_Business',\n",
    "    'user_travel_diversity', 'attraction_avg_rating_before', 'user_previous_visits',\n",
    "    'city_popularity', 'user_avg_rating_before', 'user_attraction_compatibility',\n",
    "    'sin_month', 'cos_month', 'month_mode_prob_Business', 'month_mode_prob_Family',\n",
    "    'month_mode_prob_Friends', 'month_mode_prob_Couples'\n",
    "]\n",
    "categorical_features = [\n",
    "    'VisitSeason', 'Continent', 'Region', 'Country', 'CityName',\n",
    "    'AttractionType', 'prev_visit_mode'\n",
    "]\n",
    "\n",
    "# Define target variable\n",
    "target = 'VisitMode_y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc3d409-98c2-4d89-8051-bc51c782e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "XGBoost Performance:\n",
      "Accuracy: 0.9237239313284941\n",
      "Precision (macro): 0.8910913137657148\n",
      "Recall (macro): 0.8858583495439636\n",
      "F1 Score (macro): 0.8882521402236871\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78       117\n",
      "           1       0.93      0.95      0.94      3490\n",
      "           2       0.94      0.93      0.93      2461\n",
      "           3       0.90      0.90      0.90      1851\n",
      "           4       0.91      0.85      0.88       760\n",
      "\n",
      "    accuracy                           0.92      8679\n",
      "   macro avg       0.89      0.89      0.89      8679\n",
      "weighted avg       0.92      0.92      0.92      8679\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  93    5    7   10    2]\n",
      " [   3 3311   74   79   23]\n",
      " [   6   88 2296   53   18]\n",
      " [  13   94   50 1671   23]\n",
      " [   5   44   27   38  646]]\n",
      "\n",
      "Ensemble Voting Performance:\n",
      "Accuracy: 0.9237239313284941\n",
      "Precision (macro): 0.8910913137657148\n",
      "Recall (macro): 0.8858583495439636\n",
      "F1 Score (macro): 0.8882521402236871\n",
      "Models and encoders saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing target\n",
    "df_enhanced = df_enhanced.dropna(subset=[target])\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(df_enhanced[target])\n",
    "\n",
    "# Select features (make a copy to avoid SettingWithCopyWarning)\n",
    "X = df_enhanced[numerical_features + categorical_features].copy()\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X.loc[:, col] = X[col].astype(str).fillna(\"missing\")\n",
    "    X.loc[:, col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Identify numeric and categorical for preprocessing\n",
    "numeric_feats = [col for col in X.columns if col in numerical_features]\n",
    "categorical_feats = [col for col in X.columns if col in categorical_features]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_feats)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Models (removed use_label_encoder to avoid warning)\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, random_state=42, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    predictions[name] = preds\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"Precision (macro):\", precision_score(y_test, preds, average=\"macro\"))\n",
    "    print(\"Recall (macro):\", recall_score(y_test, preds, average=\"macro\"))\n",
    "    print(\"F1 Score (macro):\", f1_score(y_test, preds, average=\"macro\"))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "    joblib.dump(pipe, f\"{name}_advanced_classifier.pkl\")\n",
    "\n",
    "# Ensemble voting (majority)\n",
    "from scipy.stats import mode\n",
    "pred_array = np.column_stack(list(predictions.values()))\n",
    "ensemble_preds = mode(pred_array, axis=1)[0].flatten()\n",
    "\n",
    "print(\"\\nEnsemble Voting Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ensemble_preds))\n",
    "print(\"Precision (macro):\", precision_score(y_test, ensemble_preds, average=\"macro\"))\n",
    "print(\"Recall (macro):\", recall_score(y_test, ensemble_preds, average=\"macro\"))\n",
    "print(\"F1 Score (macro):\", f1_score(y_test, ensemble_preds, average=\"macro\"))\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(label_encoders, \"feature_label_encoders.pkl\")\n",
    "joblib.dump(le_target, \"target_label_encoder.pkl\")\n",
    "print(\"Models and encoders saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71442bfa-a0d9-4a00-846e-48d0a5376bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
